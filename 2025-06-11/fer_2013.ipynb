{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78c2f86",
   "metadata": {},
   "source": [
    "### 얼굴 감정 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9385e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b93592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48, 48, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 48, 48, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 24, 24, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 12, 12, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12, 12, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1480615 (5.65 MB)\n",
      "Trainable params: 1478311 (5.64 MB)\n",
      "Non-trainable params: 2304 (9.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.8055 - accuracy: 0.2973\n",
      "Epoch 1: val_loss improved from inf to 1.48752, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 491s 271ms/step - loss: 1.8055 - accuracy: 0.2973 - val_loss: 1.4875 - val_accuracy: 0.4149\n",
      "Epoch 2/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795/1795 [==============================] - ETA: 0s - loss: 1.4996 - accuracy: 0.4179\n",
      "Epoch 2: val_loss improved from 1.48752 to 1.34732, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 304s 170ms/step - loss: 1.4996 - accuracy: 0.4179 - val_loss: 1.3473 - val_accuracy: 0.4812\n",
      "Epoch 3/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.3629 - accuracy: 0.4792\n",
      "Epoch 3: val_loss improved from 1.34732 to 1.27785, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 295s 165ms/step - loss: 1.3629 - accuracy: 0.4792 - val_loss: 1.2778 - val_accuracy: 0.5064\n",
      "Epoch 4/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.2915 - accuracy: 0.5127\n",
      "Epoch 4: val_loss improved from 1.27785 to 1.17879, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 584s 325ms/step - loss: 1.2915 - accuracy: 0.5127 - val_loss: 1.1788 - val_accuracy: 0.5463\n",
      "Epoch 5/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.2499 - accuracy: 0.5266\n",
      "Epoch 5: val_loss improved from 1.17879 to 1.11466, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 233s 130ms/step - loss: 1.2499 - accuracy: 0.5266 - val_loss: 1.1147 - val_accuracy: 0.5782\n",
      "Epoch 6/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.2072 - accuracy: 0.5472\n",
      "Epoch 6: val_loss did not improve from 1.11466\n",
      "1795/1795 [==============================] - 229s 127ms/step - loss: 1.2072 - accuracy: 0.5472 - val_loss: 1.1467 - val_accuracy: 0.5671\n",
      "Epoch 7/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.1674 - accuracy: 0.5607\n",
      "Epoch 7: val_loss improved from 1.11466 to 1.08991, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 233s 130ms/step - loss: 1.1674 - accuracy: 0.5607 - val_loss: 1.0899 - val_accuracy: 0.5910\n",
      "Epoch 8/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.1452 - accuracy: 0.5724\n",
      "Epoch 8: val_loss improved from 1.08991 to 1.05771, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 231s 129ms/step - loss: 1.1452 - accuracy: 0.5724 - val_loss: 1.0577 - val_accuracy: 0.6084\n",
      "Epoch 9/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.1204 - accuracy: 0.5815\n",
      "Epoch 9: val_loss did not improve from 1.05771\n",
      "1795/1795 [==============================] - 226s 126ms/step - loss: 1.1204 - accuracy: 0.5815 - val_loss: 1.0586 - val_accuracy: 0.6073\n",
      "Epoch 10/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.1054 - accuracy: 0.5898\n",
      "Epoch 10: val_loss improved from 1.05771 to 1.03411, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 237s 132ms/step - loss: 1.1054 - accuracy: 0.5898 - val_loss: 1.0341 - val_accuracy: 0.6141\n",
      "Epoch 11/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.0864 - accuracy: 0.5922\n",
      "Epoch 11: val_loss improved from 1.03411 to 1.00442, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 229s 128ms/step - loss: 1.0864 - accuracy: 0.5922 - val_loss: 1.0044 - val_accuracy: 0.6237\n",
      "Epoch 12/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.6057\n",
      "Epoch 12: val_loss improved from 1.00442 to 0.98946, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 230s 128ms/step - loss: 1.0616 - accuracy: 0.6057 - val_loss: 0.9895 - val_accuracy: 0.6325\n",
      "Epoch 13/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.0552 - accuracy: 0.6079\n",
      "Epoch 13: val_loss improved from 0.98946 to 0.97882, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 238s 133ms/step - loss: 1.0552 - accuracy: 0.6079 - val_loss: 0.9788 - val_accuracy: 0.6365\n",
      "Epoch 14/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.0398 - accuracy: 0.6139\n",
      "Epoch 14: val_loss did not improve from 0.97882\n",
      "1795/1795 [==============================] - 227s 126ms/step - loss: 1.0398 - accuracy: 0.6139 - val_loss: 1.0093 - val_accuracy: 0.6265\n",
      "Epoch 15/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.0247 - accuracy: 0.6231\n",
      "Epoch 15: val_loss did not improve from 0.97882\n",
      "1795/1795 [==============================] - 224s 125ms/step - loss: 1.0247 - accuracy: 0.6231 - val_loss: 1.0215 - val_accuracy: 0.6215\n",
      "Epoch 16/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 1.0131 - accuracy: 0.6254\n",
      "Epoch 16: val_loss improved from 0.97882 to 0.96731, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 248s 138ms/step - loss: 1.0131 - accuracy: 0.6254 - val_loss: 0.9673 - val_accuracy: 0.6428\n",
      "Epoch 17/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 0.9987 - accuracy: 0.6318\n",
      "Epoch 17: val_loss did not improve from 0.96731\n",
      "1795/1795 [==============================] - 268s 149ms/step - loss: 0.9987 - accuracy: 0.6318 - val_loss: 0.9868 - val_accuracy: 0.6353\n",
      "Epoch 18/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 0.9953 - accuracy: 0.6334\n",
      "Epoch 18: val_loss improved from 0.96731 to 0.96559, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 276s 154ms/step - loss: 0.9953 - accuracy: 0.6334 - val_loss: 0.9656 - val_accuracy: 0.6454\n",
      "Epoch 19/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 0.9833 - accuracy: 0.6384\n",
      "Epoch 19: val_loss did not improve from 0.96559\n",
      "1795/1795 [==============================] - 986s 550ms/step - loss: 0.9833 - accuracy: 0.6384 - val_loss: 0.9738 - val_accuracy: 0.6399\n",
      "Epoch 20/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 0.9669 - accuracy: 0.6395\n",
      "Epoch 20: val_loss did not improve from 0.96559\n",
      "1795/1795 [==============================] - 268s 149ms/step - loss: 0.9669 - accuracy: 0.6395 - val_loss: 0.9799 - val_accuracy: 0.6371\n",
      "Epoch 21/70\n",
      "1795/1795 [==============================] - ETA: 0s - loss: 0.9552 - accuracy: 0.6497\n",
      "Epoch 21: val_loss improved from 0.96559 to 0.95898, saving model to emotion_best.h5\n",
      "1795/1795 [==============================] - 2278s 1s/step - loss: 0.9552 - accuracy: 0.6497 - val_loss: 0.9590 - val_accuracy: 0.6470\n",
      "Epoch 22/70\n",
      " 579/1795 [========>.....................] - ETA: 2:41 - loss: 0.9461 - accuracy: 0.6534"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m\n\u001b[0;32m     71\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filename,             \u001b[38;5;66;03m# file명을 지정합니다\u001b[39;00m\n\u001b[0;32m     72\u001b[0m                              verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,            \u001b[38;5;66;03m# 로그를 출력합니다\u001b[39;00m\n\u001b[0;32m     73\u001b[0m                              save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m   \u001b[38;5;66;03m# 가장 best 값만 저장합니다\u001b[39;00m\n\u001b[0;32m     74\u001b[0m                             )\n\u001b[0;32m     75\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m---> 76\u001b[0m history\u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_val_acc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m학습종료!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m network\u001b[38;5;241m.\u001b[39msave(filename)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\deepface\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 이미지 데이터 제네레이터를 만들기\n",
    "train_generator = ImageDataGenerator(rotation_range=10, # 랜덤 회전 각도\n",
    "                                     zoom_range=0.2, # 줌인 비율\n",
    "                                     horizontal_flip=True, # 가로 반전\n",
    "                                     rescale=1/255) # 정규화 작업\n",
    "\n",
    "train_dataset = train_generator.flow_from_directory(directory='C:/Users/main/workspace/vision/2025-06-11/Fer_2013/train',\n",
    "                                                    target_size=(48,48),\n",
    "                                                    class_mode='categorical',\n",
    "                                                    batch_size=16,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=10)\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale=1/255)\n",
    "test_dataset = test_generator.flow_from_directory(directory='C:/Users/main/workspace/vision/2025-06-11/Fer_2013/test',\n",
    "                                                  target_size=(48,48),\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size=1,\n",
    "                                                  shuffle=False,\n",
    "                                                  seed=10)\n",
    "\n",
    "# FER 데이터셋은 7가지의 감정 분류를 가짐, 디텍터는 32개 이미지는 48X48 셋을 가지고 있음\n",
    "num_classes = 7\n",
    "num_detectors = 32\n",
    "width, height = 48, 48\n",
    "\n",
    "# 이를 반영하는 망을 만들어 보도록 하자\n",
    "network = Sequential()\n",
    "network.add(Conv2D(filters=num_detectors, kernel_size=3, activation='relu', padding='same', input_shape=(width, height, 3)))\n",
    "network.add(BatchNormalization())\n",
    "network.add(Conv2D(filters=num_detectors, kernel_size=3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Conv2D(2*num_detectors, 3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(Conv2D(2*num_detectors, 3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Conv2D(2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(Conv2D(2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Conv2D(2*2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(Conv2D(2*2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(2*2*num_detectors, activation='relu'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Dense(2*num_detectors, activation='relu'))\n",
    "network.add(BatchNormalization())\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Dense(num_classes, activation='softmax'))\n",
    "network.summary()\n",
    "\n",
    "\n",
    "# 이제 네트워크를 컴파일 하기\n",
    "network.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습시 오버피팅을 방지하기 위해 정확도를 모니터링하여 조기에 학습을 종료시키는 인스턴스 정의 \n",
    "monitor_val_acc = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "# loss 제일 낮을 때 가중치 저장\n",
    "filename = 'emotion_best.h5'\n",
    "checkpoint = ModelCheckpoint(filename,             # file명을 지정합니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=True   # 가장 best 값만 저장합니다\n",
    "                            )\n",
    "epochs = 70\n",
    "history= network.fit(train_dataset, epochs=epochs, validation_data=test_dataset, callbacks=[checkpoint, monitor_val_acc]).history\n",
    "print('학습종료!')\n",
    "network.save(filename)\n",
    "\n",
    "score = network.evaluate(test_dataset)\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1]*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #데이터를 분석 및 조작하기 위한 소프트웨어 라이브러리\n",
    "import matplotlib.pyplot as plt #다양한 데이터를 많은 방법으로 도식화 할 수 있도록 하는 파이썬 라이브러리\n",
    "\n",
    "plt.figure(figsize=(16,5)) \n",
    "# 만들어진 모델에 대해 train dataset과 validation dataset의 loss 를 그래프로 표현\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(history['loss']) \n",
    "plt.plot(history['val_loss']) \n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# 만들어진 모델에 대해 train dataset과 validation dataset의 accuracy 를 그래프로 표현\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(history['accuracy']) \n",
    "plt.plot(history['val_accuracy']) \n",
    "plt.title('model accuracy') \n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch') \n",
    "plt.legend(['train', 'validation'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4867cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenreator\n",
    "import os\n",
    "import cv2\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = ImageDagaGenerator(rescale=1/255)\n",
    "train_dataset = train_generator.flow_from_directory(directory='C:/Users/main/workspace/vision/2025-06-11/Fer_2013/train',\n",
    "                                                    target_size=(48,48),\n",
    "                                                    class_mode='categorical',\n",
    "                                                    batch_size=1,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=10)\n",
    "\n",
    "network = load_model('emotion_best.h5')\n",
    "\n",
    "image_list= []\n",
    "test_images = os.path.join(os.getcwd(), 'test_list')\n",
    "face_detector = dlib.cnn_face_detection_model_v1('./files/mmod_human_face_detector.dat')\n",
    "\n",
    "for root, dirs, files in os.walk(test_images):\n",
    "    for file in files :\n",
    "        if file.endswith('jpeg') or file.endswith('jpg') or file.endswith('png')\n",
    "        image_path = os.path.join(test_images, file)\n",
    "        print(image_path)\n",
    "        image_list.append(cv2.imread(image_path))\n",
    "        \n",
    "        for img in image_list:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_detector(img, 1)\n",
    "            for face_detection in faces:\n",
    "                left, top = face_detection.rect.left(), face_detection.rect.top()\n",
    "                right, bottom = face_detection.rect.right(), face_detection.rect.bottom()\n",
    "                roi = img[top:bottom, left:right]\n",
    "                roi = cv2.resize(roi, (48, 48))\n",
    "                roi = roi / 255\n",
    "                roi = np.expand_dims(roi, axis=0)\n",
    "                pred_probability = network.predict(roi)\n",
    "                print(pred_probability)\n",
    "                print(np.argmax(pred_probability))\n",
    "                print(test_dataset.class_indices)\n",
    "                i = 0\n",
    "                name = \"\"\n",
    "                for index in test_dataset.class_indices:\n",
    "                    print(index)\n",
    "                    if i == np.argmax(pred_probability):\n",
    "                        name = index\n",
    "                    i += 1\n",
    "                    \n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(img, name, (left,top), font, 1, (0,0,255), 2)\n",
    "                    cv2.rectangle(img, (left,top), (right, bottom), (0,255,0), 2)\n",
    "                    \n",
    "            cv2.imshow('Preview', img)\n",
    "            if cv2.waitKey(0) >= 0 :\n",
    "                continue\n",
    "            \n",
    "cv2.destoryAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
