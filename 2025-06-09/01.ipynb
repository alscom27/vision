{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0bb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8338e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 모델을 읽어 YOLO 구성\n",
    "def construct_yolo_v3():\n",
    "    f = open('./files/coco_names.txt', 'r')\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # cfg = 컨피그 파일\n",
    "    model = cv.dnn.readNet('./files/yolov3.weights', './files/yolov3.cfg')\n",
    "    layer_names = model.getLayerNames()\n",
    "    out_layers = [layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model, out_layers, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3295f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 모델로 물체를 검출\n",
    "def yolo_detect(img, yolo_model, out_layers):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    # print(img.shape)\n",
    "    test_img = cv.dnn.blobFromImage(img, 1.0/256, (448, 448), (0,0,0), swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3 = yolo_model.forward(out_layers)\n",
    "    \n",
    "    box, conf, id = [], [], [] # 박스, 신뢰도, 분류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores = vec85[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 : # 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx, centery = int(vec85[0]*width), int(vec85[1]*height)\n",
    "                w, h = int(vec85[2]*width), int(vec85[3]*height)\n",
    "                x, y = int(centerx - w/2), int(centery - h/2)\n",
    "                box.append([x, y, x+w, y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "                \n",
    "    ind = cv.dnn.NMSBoxes(box, conf, 0.5, 0.4)\n",
    "    objects = [box[i] + [conf[i]] + [id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "602bd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, out_layers, class_names = construct_yolo_v3() # 모델 생성\n",
    "colors = np.random.uniform(0, 255, size=(len(class_names), 3)) # 부류 마다 색깔\n",
    "\n",
    "img = cv.imread('./imgs/soccer.jpg')\n",
    "if img is None : sys.exit('파일이 없습니다.')\n",
    "\n",
    "res = yolo_detect(img, model, out_layers)\n",
    "\n",
    "for i in range(len(res)):\n",
    "    x1, y1, x2, y2, confidence, id = res[i]\n",
    "    text = str(class_names[id]) + '%.3f' %confidence\n",
    "    cv.rectangle(img,(x1, y1), (x2, y2), colors[id], 2)\n",
    "    cv.putText(img, text, (x1, y1+30), cv.FONT_HERSHEY_PLAIN, 1.5, colors[id], 2)\n",
    "    \n",
    "cv.imshow(\"Object detection by YOLO v.3\", img)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f99c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,out_layers,class_names=construct_yolo_v3()\t\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(len(class_names),3))\t# 부류마다 색깔\n",
    "\n",
    "cap=cv.VideoCapture(0,cv.CAP_DSHOW)\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    " \n",
    "    for i in range(len(res)):\n",
    "        x1,y1,x2,y2,confidence,id=res[i]\n",
    "        text=str(class_names[id])+'%.3f'%confidence\n",
    "        cv.rectangle(frame,(x1,y1),(x2,y2),colors[id],2)\n",
    "        cv.putText(frame,text,(x1,y1+30),cv.FONT_HERSHEY_PLAIN,1.5,colors[id],2)\n",
    "    \n",
    "    cv.imshow(\"Object detection from video by YOLO v.3\",frame)\n",
    "    \n",
    "    key=cv.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "    \n",
    "cap.release()\t\t# 카메라와 연결을 끊음\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa563db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리한 프레임 수= 66 , 경과 시간= 21.97389817237854 \n",
      "초당 프레임 수= 3.0035635681138637\n"
     ]
    }
   ],
   "source": [
    "model,out_layers,class_names=construct_yolo_v3()\t\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(len(class_names),3))\t# 부류마다 색깔\n",
    "\n",
    "cap=cv.VideoCapture(0,cv.CAP_DSHOW)\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "import time\n",
    "\n",
    "start=time.time()\n",
    "n_frame=0\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    " \n",
    "    for i in range(len(res)):\n",
    "        x1,y1,x2,y2,confidence,id=res[i]\n",
    "        text=str(class_names[id])+'%.3f'%confidence\n",
    "        cv.rectangle(frame,(x1,y1),(x2,y2),colors[id],2)\n",
    "        cv.putText(frame,text,(x1,y1+30),cv.FONT_HERSHEY_PLAIN,1.5,colors[id],2)\n",
    "    \n",
    "    cv.imshow(\"Object detection from video by YOLO v.3\",frame)\n",
    "    n_frame+=1\n",
    "    \n",
    "    key=cv.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "\n",
    "end=time.time()\n",
    "print('처리한 프레임 수=',n_frame,', 경과 시간=',end-start,'\\n초당 프레임 수=',n_frame/(end-start))\n",
    "\n",
    "cap.release()\t\t# 카메라와 연결을 끊음\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff619380",
   "metadata": {},
   "source": [
    "### pixellib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7649c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixellib.semantic import semantic_segmentation\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ce6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Image saved successfuly in your current working directory.\n"
     ]
    }
   ],
   "source": [
    "seg = semantic_segmentation()\n",
    "seg.load_ade20k_model('./files/deeplabv3_xception65_ade20k.h5')\n",
    "\n",
    "img_fname = './imgs/busy_street.jpg'\n",
    "seg.segmentAsAde20k(img_fname, output_image_name='./imgs/image_new.jpg')\n",
    "info1, img_segmented1 = seg.segmentAsAde20k(img_fname)\n",
    "info2, img_segmented2 = seg.segmentAsAde20k(img_fname, overlay=True)\n",
    "\n",
    "cv.imshow('Image original', cv.imread(img_fname))\n",
    "cv.imshow('Image segmention', img_segmented1)\n",
    "cv.imshow('Image segmention overlayed', img_segmented2)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f17b70",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m seg_video\u001b[38;5;241m=\u001b[39msemantic_segmentation()\n\u001b[0;32m      4\u001b[0m seg_video\u001b[38;5;241m.\u001b[39mload_ade20k_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./files/deeplabv3_xception65_ade20k.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mseg_video\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_camera_ade20k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcap\u001b[49m\u001b[43m,\u001b[49m\u001b[43moverlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mframes_per_second\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moutput_video_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_video.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshow_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mframe_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPixellib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m cv\u001b[38;5;241m.\u001b[39mwaitKey()\n\u001b[0;32m      9\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\pixellib\\semantic.py:555\u001b[0m, in \u001b[0;36msemantic_segmentation.process_camera_ade20k\u001b[1;34m(self, cam, overlay, check_fps, frames_per_second, output_video_name, show_frames, frame_name, verbose)\u001b[0m\n\u001b[0;32m    553\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m capture\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[1;32m--> 555\u001b[0m   raw_labels, frame_overlay  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmentAsAde20k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m   counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    557\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m show_frames \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\pixellib\\semantic.py:411\u001b[0m, in \u001b[0;36msemantic_segmentation.segmentAsAde20k\u001b[1;34m(self, image_path, output_image_name, overlay, process_frame, verbose)\u001b[0m\n\u001b[0;32m    409\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing image....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m#run prediction\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(res\u001b[38;5;241m.\u001b[39msqueeze(), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# remove padding and resize back to original image\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1749\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1748\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 1749\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1751\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    926\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\main\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap=cv.VideoCapture(0)\n",
    "\n",
    "seg_video=semantic_segmentation()\n",
    "seg_video.load_ade20k_model('./files/deeplabv3_xception65_ade20k.h5')\n",
    "\n",
    "seg_video.process_camera_ade20k(cap,overlay=True,frames_per_second=2,output_video_name='output_video.mp4',show_frames=True,frame_name='Pixellib')\n",
    "\n",
    "cap.release()\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c85626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 13 persons, 7 bicycles, 3 cars, 2 traffic lights, 8 backpacks, 3 handbags, 30.1ms\n",
      "Speed: 1.3ms preprocess, 30.1ms inference, 2.8ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO('./files/yolov8x-seg.pt')\n",
    "img = cv2.imread('./imgs/busy_street.jpg')\n",
    "results = model(img)\n",
    "\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e78af",
   "metadata": {},
   "source": [
    "### pixellib 활용한 배경 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477c6b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BatchNormalization' from 'tensorflow.python.keras.layers' (c:\\Users\\main\\miniconda3\\envs\\dl\\Lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyQt5\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mQtWidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpixellib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtune_bg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m alter_bg\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\miniconda3\\envs\\dl\\Lib\\site-packages\\pixellib\\tune_bg\\__init__.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpixellib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msemantic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeeplab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Deeplab_xcep_pascal\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpixellib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msemantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m obtain_segmentation\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\miniconda3\\envs\\dl\\Lib\\site-packages\\pixellib\\semantic\\__init__.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeeplab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Deeplab_xcep_pascal\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeeplab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Deeplab_xcep_ade20k\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\miniconda3\\envs\\dl\\Lib\\site-packages\\pixellib\\semantic\\deeplab.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Add\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dropout\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchNormalization\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2D\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DepthwiseConv2D\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BatchNormalization' from 'tensorflow.python.keras.layers' (c:\\Users\\main\\miniconda3\\envs\\dl\\Lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PyQt5.QtWidgets import *\n",
    "import sys\n",
    "from pixellib.tune_bg import alter_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoSpecialEffect(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle('배경을 내 맘대로')\n",
    "        self.setGeometry(200, 200, 400, 100)\n",
    "        \n",
    "        VideoButton = QPushButton('배경 내 맘대로 켜기', self)\n",
    "        self.pickCombo = QComboBox(self)\n",
    "        self.pickCombo.addItems(['원래 영상', '흐릿(조금)', '흐릿(중간)', '흐릿(많이)',\n",
    "                                 '빨강', '녹색', '파랑'])\n",
    "        quitButton = QPushButton('나가기', self)\n",
    "        \n",
    "        VideoButton.setGeometry(10, 10, 140, 30)\n",
    "        self.pickCombo.setGeometry(150, 10, 110, 30)\n",
    "        quitButton.setGeometry(280, 10, 100, 30)\n",
    "        \n",
    "        VideoButton.clicked.connect(self.videoSpecialEffectFunction)\n",
    "        quitButton.clicked.connect(self.quitFunction)\n",
    "        \n",
    "    def videoSpecialEffectFunction(self):\n",
    "        self.cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "        if not self.cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "        \n",
    "        while True:\n",
    "            ret,frame=self.cap.read()  \n",
    "            if not ret: break\n",
    "\n",
    "            pick_effect=self.pickCombo.currentIndex()        \n",
    "            if pick_effect==0:\n",
    "                special_img=frame\n",
    "            elif pick_effect==1:\n",
    "                special_img=change_bg.blur_frame(frame,low=True,detect='person')\n",
    "            elif pick_effect==2:\n",
    "                special_img=change_bg.blur_frame(frame,moderate=True,detect='person')\n",
    "            elif pick_effect==3:    \n",
    "                special_img=change_bg.blur_frame(frame,extreme=True,detect='person')\n",
    "            elif pick_effect==4:    \n",
    "                special_img=change_bg.color_frame(frame,colors=(255,0,0),detect='person')\n",
    "            elif pick_effect==5:    \n",
    "                special_img=change_bg.color_frame(frame,colors=(0,255,0),detect='person')\n",
    "            elif pick_effect==6:    \n",
    "                special_img=change_bg.color_frame(frame,colors=(0,0,255),detect='person')\n",
    "                \n",
    "            cv.imshow('Special effect',special_img)              \n",
    "            cv.waitKey(1) \n",
    "                                        \n",
    "    def quitFunction(self):\n",
    "        self.cap.release()\n",
    "        cv.destroyAllWindows()        \n",
    "        self.close()\n",
    "\n",
    "change_bg=alter_bg(model_type=\"pb\")\n",
    "change_bg.load_pascalvoc_model('xception_pascalvoc.pb')\n",
    "                \n",
    "app=QApplication(sys.argv) \n",
    "win=VideoSpecialEffect() \n",
    "win.show()\n",
    "app.exec_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
